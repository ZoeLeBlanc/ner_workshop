{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd9f709",
   "metadata": {},
   "source": [
    "## Welcome to Introduction to Named Entity Recognition ðŸŽ‰\n",
    "\n",
    "This is our first notebook and will introduce the following topics:\n",
    "\n",
    "- How to preprocess textual data for NER\n",
    "- How to implement basic NER pipelines\n",
    "- How to use the basics of nltk and spacy libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55df737",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cedcd51",
   "metadata": {},
   "source": [
    "We've discussed NER at 30000 ft level but how do we actually get a computer to help us take unstructured textual data and recognize named entities?\n",
    "\n",
    "Well first place to start is with how a computer even matches strings in the first place (sorry if this is repeat info but always good to go over the foundations!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import our first libraries for today. We'll be importing more later but first we will be using pandas and Python's regular expression library\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19bc34e",
   "metadata": {},
   "source": [
    "Today we'll be using two datasets. The first is very tiny and a bit silly, but that's never a bad way to start a workshop!\n",
    "\n",
    "The dataset is a collection of movie scripts and metadata for the first three Harry Potter films (available publicly here on Kaggle https://www.kaggle.com/gulsahdemiryurek/harry-potter-dataset and apparently scraped from the potter wiki and pottermore.com).\n",
    "\n",
    "I usually teach NER using a web scraped version of the Humanist Listserv, but since I used it last week, I decided to mix it up a bit and William Mattingly uses a similar dataset in his videos on NER and DH, which I highly recommend and you can find them here https://pythonhumanities.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c731757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in and inspect our data starting with the Characters spreadsheet\n",
    "chars_df = pd.read_csv('./archive/Characters.csv', delimiter=';')\n",
    "chars_df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also load in the first film script\n",
    "film1_df = pd.read_csv('./archive/Harry Potter 1.csv', delimiter=';')\n",
    "film1_df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58ee13",
   "metadata": {},
   "source": [
    "Now looking at these two files, you can immediately see that `chars_df` holds our metadata, whereas `film1_df` holds two columns representing the character speaking and their speech.\n",
    "\n",
    "So why would NER be helpful for extracting information from this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5495c",
   "metadata": {},
   "source": [
    "![](https://newleftreview.org/system/dragonfly/production/2019/05/13/4k9ql9wxyf_3020501large.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eecd43",
   "metadata": {},
   "source": [
    "This figure is from Stanford's Literary Lab Pamphlet exploring network analysis of Hamlet's plot. You can read more about the experiment here https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf or read a similar network analysis here by James Lee https://dasil.sites.grinnell.edu/2014/11/a-network-analysis-of-shakespeares-plays/\n",
    "\n",
    "But essentially we can use NER to help us extract when a character speaks the name of another character. So let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257f205",
   "metadata": {},
   "source": [
    "First, let's start by browsing the documentation on string manipulation with Pandas https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html. \n",
    "\n",
    "(If you need a more in-depth refresher checkout Melanie Walsh's notebooks from the earlier workshop on Data Analysis with Pandas https://github.com/melaniewalsh/Data-Analysis-with-Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43880f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How might we do string matching based on pandas docs?\n",
    "# film1_df[film1_df.Sentence.str.contains(chars_df[0:1].Name.values[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf611a27",
   "metadata": {},
   "source": [
    "Why didn't the above code work? Should we try a different method? Or is something wrong with the underlying data ðŸ¤”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Let's inspect chars_df\n",
    "# chars_df[0:1].Name.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe0053",
   "metadata": {},
   "source": [
    "We can solve this issue a few ways, let's start with splitting names apart so we can search for first and last names separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_df['split_names'] = chars_df.Name.str.split(' ')\n",
    "\n",
    "film1_df[film1_df.Sentence.str.contains('|'.join(chars_df[0:1].split_names.values[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5276be",
   "metadata": {},
   "source": [
    "Great we're getting results! I realize the syntax is a little wonky but essentially we're splitting names and them joining them back together with the pipe operator so that pandas knows to check for any of them within the sentence (so in this case `Harry`, `James`, or `Potter` exists in 114 rows).\n",
    "\n",
    "Let's try and get this code working with all our character names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(row):\n",
    "    character_names = chars_df.split_names.tolist()\n",
    "    identified_names = []\n",
    "    for names in character_names:\n",
    "        if any(name in row.Sentence for name in names):\n",
    "            identified_names.append(' '.join(names))\n",
    "    row['identified_names'] = identified_names\n",
    "    return row\n",
    "film1_df = film1_df.apply(find_entities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect our results\n",
    "film1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d68f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "film1_df = film1_df[film1_df.identified_names.astype(bool)]\n",
    "film1_exploded = film1_df.explode('identified_names')\n",
    "film1_exploded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2a9af",
   "metadata": {},
   "source": [
    "This looks pretty close to what we want. We might eventually try and reconcile the names in `Character` and `identified_names`, but we've know officially run our first NER algorithm ðŸŽ‰.\n",
    "\n",
    "But wait... let's inspect a little closer ðŸ¤”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "film1_exploded[film1_exploded.Sentence.str.contains('James')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ddebc4",
   "metadata": {},
   "source": [
    "We can see in this example that while we are correctly identifying Lily and James Potter, we're also returning values for Harry and Albus Potter. This makes sense since we're only checking if **any** of the strings in the names match. If we try to match the whole exact name we end up back we're we started, since few characters say the full name of the character they are either speaking  or referring to.\n",
    "\n",
    "There's a few solutions we could implement.\n",
    "\n",
    "We could for example try and remove middle names since they are used infrequently and we could even write a function that uses distance metrics to weight our matches. \n",
    "\n",
    "But instead of going deeper here let's zoom out for a second.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa2916",
   "metadata": {},
   "source": [
    "#### Pre-Processing and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8210e",
   "metadata": {},
   "source": [
    "So far we've used a list of characters metadata, separated the names by spaces, and then checked if those names matched, which gave us some preliminary data but still some errors as well.\n",
    "\n",
    "All of these steps might feel small, but they actually represent big interpretative choices. \n",
    "\n",
    "While these types of choices are often called *data cleaning* or *pre-processing*, they are all influential because each choice means we are altering our underlying dataset to prioritize algorithmic tractability.\n",
    "\n",
    "In \"Evaluating the Stability of Embedding-based Word Similarities\", Maria Antoniak and David Mimno describe the differing research goals of those interested in using NLP methods for *downstream-centered* versus *corpus-centered* research. \n",
    "\n",
    "![](./images/corpus_downstream.png)\n",
    "\n",
    "I often find this distinction is an important one to reinforce because for many tutorials and resources available, they assume that we are interested in downstream (that is we are using NLP methods and a corpus to either optimize our method or develop some downstream application like a search engine). However, most humanists are actually corpus-centric. We care about our data and representing it as accurately as possible, which means every pre-processing step needs to be considered carefully. So let's discuss some of these choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e164f",
   "metadata": {},
   "source": [
    "##### Tokenization\n",
    "\n",
    "In the field of corpus linguistics, the term â€œwordâ€ is generally dispensed with as too abstract in favor of the idea of a â€œtokenâ€ or â€œtype.â€ Breaking a piece of text into words is thus called â€œtokenization.â€\n",
    "\n",
    "![tokenize](https://blog.floydhub.com/content/images/2020/02/tokenize.png)\n",
    "\n",
    "While so far we've tokenized on spaces, this image shows that we could do punctuation or even our set of rules depending on our goals and also our language of interest. Here's a deeper dive into other forms of tokenization for those interested https://blog.floydhub.com/tokenization-nlp/.\n",
    "\n",
    "In our example above, instead of tokenizing the names into unigrams, we might have tried bigrams instead to make sure we were catching full names as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa329dfa",
   "metadata": {},
   "source": [
    "##### Lemmatizing and Stemming \n",
    "\n",
    "In addition to tokenization, the other big choice we'll be making with NER is if we want to employ lemmatizing or stemming to our text to help identify entities and also limit the number of potential similar options. \n",
    "\n",
    "![stemming](https://miro.medium.com/max/1400/1*-MTbZK9ha3Kp1Z50o79Tzg.png)\n",
    "\n",
    "![lemma](https://devopedia.org/images/article/227/6785.1570815200.png)\n",
    "\n",
    "These choices will help normalize our data, which might work well if we're dealing with either OCR or human-entered data (both of which are notoriously error prone).\n",
    "\n",
    "Here is some code for actually implementing both stemming and lemmatizing.\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "```\n",
    "First import the Porter Stemmer from NLTK, which is one of the more common stemming approaches. We can try out the stemmer using the `stem()` method on our text and then test if it is equal to the initial `text`.\n",
    "```python\n",
    "stemmed = porter.stem(text)\n",
    "print(stemmed == text)\n",
    "```\n",
    "We can test it out on our text, but really we want to use it on each row, so let's create a new function called `stem_words`.\n",
    "```python\n",
    "def stem_words(row):\n",
    "    stemmed_words = ''\n",
    "    for token in row.split(' '):\n",
    "        stemmed_words += porter.stem(token) + ' '\n",
    "    return stemmed_words\n",
    "film1_df['stemmed_sentence'] = film1_df.Sentence.apply(stem_words)\n",
    "```\n",
    "\n",
    "We can also import the `WordNetLemmatizer` from NLTK to also test out lemmatizing our text, and then create a second function to lemmatize the text.\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_words(row):\n",
    "    lemmatized_words = ''\n",
    "    for token in row.split(' '):\n",
    "        lemmatized_words += wordnet_lemmatizer.lemmatize(token) + ' '\n",
    "    return lemmatized_words\n",
    "film1_df['lemmatized_sentence'] = film1_df.Sentence.apply(lemma_words)\n",
    "```\n",
    "\n",
    "\n",
    "Another option is to use more complex string matching. For example, there's a popular Python library called `Fuzzy Wuzzy` that uses levenshtein distance to calculate string matches https://github.com/seatgeek/fuzzywuzzy.\n",
    "\n",
    "![fuzzywuzzy](./images/fuzzywuzzy.png)\n",
    "\n",
    "Or we might try the library `Textpack` that uses TFIDF weighting to also help normalize data across columns https://github.com/lukewhyte/textpack.\n",
    "\n",
    "![textpack](./images/textpack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b50ba5",
   "metadata": {},
   "source": [
    "One final thing we could do to normalize our data is try lowercasing both the character names and our film data to maximize matches (especially since we didn't actually scrape this data ourselves and so there might be inconsistencies or we might even be working with languages where there is no captialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "film1_df['sentence_lower'] = film1_df.Sentence.str.lower()\n",
    "chars_df['name_lower'] = chars_df.Name.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c13bd",
   "metadata": {},
   "source": [
    "At this stage we could re-run our code, but one thing to think about is whether lowercasing our strings is not only going to get us more matches, but also more errors.\n",
    "\n",
    "For example, the name `Potter` might refer to at least three characters, but it is also both a verb and noun in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061aa086",
   "metadata": {},
   "source": [
    "***\n",
    "#### The Challenges of Unstructured Text\n",
    "\n",
    "The example above highlights one of the struggles of working with unstructured textual data and NER - that is many meanings of words are contextual.  \n",
    "\n",
    "Our example of `Potter` is an instance of *polysemy* - that is a word that has multiple, related meanings (another great example is the word `book`).\n",
    "\n",
    "We also have the issue of *homonymy*, when words are spelt the same but have completely distinct meanings (think of words like `bat` or `bank`).\n",
    "\n",
    "Finally we have *metonymy* where words are replaced with another reference but have the same meaning. A common example is businessman with suits, but in our dataset probably the best example is Voldemort with He-Who-Must-Not-Be-Named.\n",
    "\n",
    "NLP has developed lots of approaches for dealing with these issues (think of our slide listing NLP and NLU for example), but let's focus on seeing how NER can help us tackle these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8185d1",
   "metadata": {},
   "source": [
    "#### NER Python Libraries\n",
    "\n",
    "There are multiple libraries for NER in Python, and evaluating which one is right for your project and data depends on a few different factors. Each of these libraries has its own history, and some of what they provide overlaps. Here's a helpful chart outlining some of their pros and cons.\n",
    "\n",
    "![comparison](https://activewizards.com/content/blog/Comparison_of_Python_NLP_libraries/nlp-librares-python-prs-and-cons01.png)\n",
    "\n",
    "The main ones for NER particularly are the following:\n",
    "\n",
    "- NLTK [https://www.nltk.org/](https://www.nltk.org/)\n",
    "\n",
    "- spaCy [https://spacy.io/](https://spacy.io/)\n",
    "\n",
    "- STANZA https://stanfordnlp.github.io/stanza/\n",
    "\n",
    "- SPARK NLP https://www.johnsnowlabs.com/spark-nlp/\n",
    "\n",
    "\n",
    "I was initially planning to start with NLTK since it was largely the original (started in 2001), but I was having some dependency errors for their NER module (`ne_chunk`). So if you want to try it out yourself feel free to check out the instructions in the NLTK book https://www.nltk.org/book/ch07.html (look for 5   Named Entity Recognition\n",
    ").\n",
    "\n",
    "Instead, let's start off with spaCy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27939d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First let's download the library\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d7f64",
   "metadata": {},
   "source": [
    "We then need to download a model to work with spaCy so let's take a look at the model options and documentation https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Then let's download our spacy model\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aba2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can import Spacy and the model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Or use the default model, which has fewer features:\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40465668",
   "metadata": {},
   "source": [
    "Let's take a look at the spaCy implementation https://spacy.io/usage/spacy-101#annotations-ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c649b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc), doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6586ef0",
   "metadata": {},
   "source": [
    "What is a type `Doc`? [https://spacy.io/api/doc](https://spacy.io/api/doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c855521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's dig into what this Class gives us\n",
    "[prop for prop in dir(doc) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854286aa",
   "metadata": {},
   "source": [
    "We also have the Class `Token` [https://spacy.io/api/token](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_word = doc[0]\n",
    "type(first_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45496224",
   "metadata": {},
   "outputs": [],
   "source": [
    "[prop for prop in dir(first_word) if not prop.startswith('__')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ffa78",
   "metadata": {},
   "source": [
    "So let's try this out with our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "film1_doc = nlp(film1_df[0:1].Sentence.values[0])\n",
    "film1_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could use spaCy for parts-of-speech tagging of this sentence\n",
    "for token in film1_doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc72e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(film1_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f13275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(film1_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e534a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_doc = nlp(film1_df[film1_df.Sentence.str.contains('London')][1:2].Sentence.values[0])\n",
    "london_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(london_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4443f",
   "metadata": {},
   "source": [
    "![spacy_entities](https://devopedia.org/images/article/256/8660.1580659054.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02793bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# So let's try and write a function to identify all Named Entities in our script\n",
    "identified_entities = []\n",
    "def find_spacy_entities(row):\n",
    "    # code goes here\n",
    "    spacy_sentence = nlp(row.Sentence)\n",
    "    \n",
    "    list_tokens = []\n",
    "    list_entities = []\n",
    "    for token in spacy_sentence.ents:\n",
    "        list_tokens.append(token.text)\n",
    "        list_entities.append(token.label_)\n",
    "    row['spacy_tokens'] = list_tokens\n",
    "    row['spacy_entities'] = list_entities\n",
    "    return row\n",
    "\n",
    "film1_df = film1_df.apply(find_spacy_entities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "film1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280f92b",
   "metadata": {},
   "source": [
    "#### Rules-Based NER and Gazetteers\n",
    "\n",
    "So let's compare spaCy to what we had originally. You can see that spaCy is identifying for more entities than we did with the characters which might be useful depending on our goals, but for our network example we want the characters not just any entities.\n",
    "\n",
    "One way to think of our characters metadata is as a gazetteer.\n",
    "\n",
    "Gazetteers are a popular method in DH of dealing with variation in NER and identifying historical entities. You can look at the World Historical Gazetteer as an example http://whgazetteer.org/.\n",
    "\n",
    "With our characters list as a gazetteer, we could just keep trying to write more and more complex for loops, but alternatively we could join spaCy with this gazetteer by adding in new rules to our model.\n",
    "\n",
    "Rules-based NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00491923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [\n",
    "                {\"label\": \"PERSON\", \"pattern\": \"McGonagall\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(film1_df[0:1].Sentence.values[0])\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebe71d",
   "metadata": {},
   "source": [
    "We can read more about rules based NER in William's textbook http://ner.pythonhumanities.com/02_rules_based_ner.html and the spacy documentation https://spacy.io/api/entityruler but for now let's try together to add in our remaining characters and see if we can improve Spacy's functionality for our goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c0ab2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ebaaa",
   "metadata": {},
   "source": [
    "#### Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773add8",
   "metadata": {},
   "source": [
    "If you quickly get through working with the Harry Potter dataset, then let's try working with a new more challenging dataset.\n",
    "\n",
    "In the `additional_data` folder, you'll see that there's a set of spreadsheets and text files. These datasets were pulled from the ParlaMint 2.1 database https://www.clarin.si/repository/xmlui/handle/11356/1432. \n",
    "\n",
    "> ParlaMint 2.1 is a multilingual set of 17 comparable corpora containing parliamentary debates mostly starting in 2015 and extending to mid-2020, with each corpus being about 20 million words in size. The sessions in the corpora are marked as belonging to the COVID-19 period (after November 1st 2019), or being \"reference\" (before that date)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f4188",
   "metadata": {},
   "source": [
    "So you're goal for these datasets is to:\n",
    "1. load in the data and combine the text with the metadata\n",
    "2. see if you can get spaCy to identify entities\n",
    "3. try working with both the English and French language data (hint remember to load in the correct spacy model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea80ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_hum_env",
   "language": "python",
   "name": "ner_hum_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
