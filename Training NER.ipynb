{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Custom NER Models\n",
    "\n",
    "Today's Goals/Agenda:\n",
    "- Review Wednesday materials and our deeper diver into spaCy\n",
    "- Discussion of how we evaluate NER models and libraries\n",
    "- Building and training custom NER models\n",
    "\n",
    "Final afternoon of TAPI and NER workshop 🥳😳!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download Spacy in Binder\n",
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download Spacy if running locally\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some of our original code from Monday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our data\n",
    "chars_df = pd.read_csv('./archive/Characters.csv', delimiter=';')\n",
    "chars_df['split_names'] = chars_df.Name.str.split(' ')\n",
    "film1_df = pd.read_csv('./archive/Harry Potter 1.csv', delimiter=';')\n",
    "\n",
    "def find_entities(row):\n",
    "    # Find character names from chars_df\n",
    "    character_names = chars_df.split_names.tolist()\n",
    "    identified_names = []\n",
    "    for names in character_names:\n",
    "        if any(name in row.Sentence for name in names):\n",
    "            identified_names.append(' '.join(names))\n",
    "    row['identified_names'] = identified_names\n",
    "    return row\n",
    "\n",
    "film1_entities = film1_df.apply(find_entities, axis=1)\n",
    "film1_entities = film1_entities[film1_entities.identified_names.astype(bool)]\n",
    "film1_exploded = film1_entities.explode('identified_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our code from Wednesday 👇🏽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example rule {'label': 'PERSON', 'pattern': [{'LOWER': 'parvati'}], 'id': 'parvati'}\n",
      "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'sents_p': 1.0, 'sents_r': 1.0, 'sents_f': 1.0, 'tag_acc': None, 'pos_acc': None, 'morph_acc': None, 'morph_per_feat': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': 1.0, 'ents_r': 1.0, 'ents_f': 1.0, 'ents_per_type': {'PERSON': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'CARDINAL': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'DATE': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'TIME': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'ORDINAL': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'NORP': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'QUANTITY': {'p': 1.0, 'r': 1.0, 'f': 1.0}}, 'cats_score': 0.0, 'cats_score_desc': 'macro F', 'cats_micro_p': 0.0, 'cats_micro_r': 0.0, 'cats_micro_f': 0.0, 'cats_macro_p': 0.0, 'cats_macro_r': 0.0, 'cats_macro_f': 0.0, 'cats_macro_auc': 0.0, 'cats_f_per_type': {}, 'cats_auc_per_type': {}}\n"
     ]
    }
   ],
   "source": [
    "# Load film datasets\n",
    "film1_df = pd.read_csv('./archive/Harry Potter 1.csv', delimiter=';')\n",
    "film2_df = pd.read_csv('./archive/Harry Potter 2.csv', delimiter=';')\n",
    "film3_df = pd.read_csv('./archive/Harry Potter 3.csv', delimiter=';')\n",
    "\n",
    "# Combine film dataframes\n",
    "film3_df.columns = map(str.capitalize, film3_df.columns)\n",
    "film1_df['movie_number'] = 'film 1'\n",
    "film2_df['movie_number'] = 'film 2'\n",
    "film3_df['movie_number'] = 'film 3'\n",
    "films_df = pd.concat([film1_df, film2_df, film3_df])\n",
    "\n",
    "chars_df['full_names'] = np.where(\n",
    "    chars_df.split_names.str.len() == 2, \n",
    "    chars_df.split_names.str[0].str.lower() + ' ' + chars_df.split_names.str[1].str.lower(), \n",
    "    np.where(\n",
    "        chars_df.split_names.str.len() > 2,\n",
    "        chars_df.split_names.str[0].str.lower() + ' ' + chars_df.split_names.str[-1].str.lower(),\n",
    "        chars_df.split_names.str[0].str.lower())) \n",
    "chars_df['first_name'] = chars_df.split_names.str[0].str.lower()\n",
    "chars_df['last_name'] = chars_df.split_names.str[-1].str.lower()\n",
    "\n",
    "full_names = chars_df.full_names.unique().tolist()\n",
    "first_names = chars_df.first_name.unique().tolist()\n",
    "last_names = chars_df.last_name.unique().tolist()\n",
    "\n",
    "# Get unique names and create our rules\n",
    "names = list(set(first_names) | set(last_names))\n",
    "unique_names = list(set(names) | set(full_names))\n",
    "list_names = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": f\"{name}\"}], \"id\": f\"{name}\"} for name in unique_names if len(name.split(' ')) == 1]\n",
    "\n",
    "list_full_names = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": f\"{name.split(' ')[0]}\"}, {\"LOWER\": f\"{name.split(' ')[1]}\"}], \"id\": f\"{'-'.join(name.split(' '))}\"} for name in full_names if len(name.split(' ')) > 1]\n",
    "\n",
    "all_names = list_names + list_full_names\n",
    "print('example rule', all_names[0])\n",
    "\n",
    "# Load our models and pass our rules\n",
    "full_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = full_nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(all_names)\n",
    "\n",
    "blank_nlp = spacy.blank(\"en\")\n",
    "ruler = blank_nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(all_names)\n",
    "\n",
    "evaluation_data = []\n",
    "def evaluate_spacy_models(row):\n",
    "    # Let's also add in blank_nlp\n",
    "    sentences = nltk.sent_tokenize(row.Sentence.lower())\n",
    "    for sentence in sentences:\n",
    "        spacy_full = full_nlp(sentence)\n",
    "        list_entities = []\n",
    "        for token in spacy_full.ents:\n",
    "            list_entities.append([token.start_char, token.end_char, token.label_])\n",
    "        if len(list_entities) > 0:\n",
    "            entry = (sentence,{\"entities\": list_entities})\n",
    "            evaluation_data.append(entry)\n",
    "    return row\n",
    "films_df.apply(evaluate_spacy_models, axis=1)\n",
    "\n",
    "def evaluate(ner_model, testing_data):\n",
    "    scorer = Scorer()\n",
    "    examples = []\n",
    "    for input_, annot in testing_data:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        example = Example.from_dict(doc_gold_text, annot)\n",
    "        example.predicted = ner_model(input_)\n",
    "        examples.append(example)\n",
    "        \n",
    "    print(scorer.score(examples))\n",
    "\n",
    "# print the results\n",
    "evaluate(full_nlp, evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blank Harry Potter PERSON\n",
      "blank Harry PERSON\n",
      "blank Weasley PERSON\n",
      "blank Tom PERSON\n",
      "blank Riddle PERSON\n",
      "blank Tom Riddle PERSON\n",
      "blank Gilderoy Lockhart PERSON\n",
      "blank Harry PERSON\n",
      "blank Harry PERSON\n",
      "blank Hermione PERSON\n",
      "blank Harry PERSON\n",
      "blank Harry PERSON\n",
      "blank Lucius Malfoy PERSON\n",
      "blank Draco PERSON\n",
      "full Harry Potter PERSON\n",
      "full the Chamber of Secrets ORG\n",
      "full Harry PERSON\n",
      "full second year DATE\n",
      "full Hogwarts ORG\n",
      "full 50-year-old DATE\n",
      "full Ron PERSON\n",
      "full Ginny Weasley PERSON\n",
      "full her first year DATE\n",
      "full Hogwarts ORG\n",
      "full Tom Marvolo Riddle PERSON\n",
      "full World War II EVENT\n",
      "full Voldemort ORG\n",
      "full Tom Riddle PERSON\n",
      "full Ginny PERSON\n",
      "full Voldemort ORG\n",
      "full Ginny ORG\n",
      "full Voldemort ORG\n",
      "full Hogwarts PRODUCT\n",
      "full Defence Against the Dark Arts ORG\n",
      "full Gilderoy Lockhart PERSON\n",
      "full Harry PERSON\n",
      "full the Wizarding World LOC\n",
      "full Voldemort PERSON\n",
      "full Muggles PERSON\n",
      "full Harry PERSON\n",
      "full the Dark Arts ORG\n",
      "full Hermione PERSON\n",
      "full Harry PERSON\n",
      "full Ron PERSON\n",
      "full the Chamber of Secrets ORG\n",
      "full Harry PERSON\n",
      "full Ginny PERSON\n",
      "full Voldemort ORG\n",
      "full Lucius Malfoy PERSON\n",
      "full Draco ORG\n",
      "full Ron PERSON\n",
      "full Ginny PERSON\n",
      "full Ginny PERSON\n"
     ]
    }
   ],
   "source": [
    "# https://en.wikipedia.org/wiki/Harry_Potter\n",
    "test_text = \"The series continues with Harry Potter and the Chamber of Secrets, describing Harry's second year at Hogwarts. He and his friends investigate a 50-year-old mystery that appears uncannily related to recent sinister events at the school. Ron's younger sister, Ginny Weasley, enrols in her first year at Hogwarts, and finds an old notebook in her belongings which turns out to be the diary of a previous student, Tom Marvolo Riddle, written during World War II. He is later revealed to be Voldemort's younger self, who is bent on ridding the school of 'mudbloods', a derogatory term describing wizards and witches of non-magical parentage. The memory of Tom Riddle resides inside of the diary and when Ginny begins to confide in the diary, Voldemort is able to possess her. Through the diary, Ginny acts on Voldemort's orders and unconsciously opens the 'Chamber of Secrets', unleashing an ancient monster, later revealed to be a basilisk, which begins attacking students at Hogwarts. It kills those who make direct eye contact with it and petrifies those who look at it indirectly. The book also introduces a new Defence Against the Dark Arts teacher, Gilderoy Lockhart, a highly cheerful, self-conceited wizard with a pretentious facade, later turning out to be a fraud. Harry discovers that prejudice exists in the Wizarding World through delving into the school's history, and learns that Voldemort's reign of terror was often directed at wizards and witches who were descended from Muggles. Harry also learns that his ability to speak the snake language Parseltongue is rare and often associated with the Dark Arts. When Hermione is attacked and petrified, Harry and Ron finally piece together the puzzles and unlock the Chamber of Secrets, with Harry destroying the diary for good and saving Ginny, and, as they learn later, also destroying a part of Voldemort's soul. The end of the book reveals Lucius Malfoy, Draco's father and rival of Ron and Ginny's father, to be the culprit who slipped the book into Ginny's belongings.\"\n",
    "\n",
    "doc = blank_nlp(test_text)\n",
    "for token in doc.ents:\n",
    "    print('blank', token.text, token.label_)\n",
    "\n",
    "doc = full_nlp(test_text)\n",
    "for token in doc.ents:\n",
    "    print('full', token.text, token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save spaCy models to disk, use the following syntax. Be careful though to not name it the same as a downloaded spaCy model like 'en_core_web_sm' because that will overwrite that model. You can read more here https://spacy.io/usage/saving-loading\n",
    "blank_nlp.to_disk('blank_nlp')\n",
    "full_nlp.to_disk('full_nlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Training a Custom NER Model\n",
    "\n",
    "A few things to keep in mind:\n",
    "- you can use pre-trained NER models off the shelf and those might be close enough to what you need\n",
    "While custom training is not always the answer, it might be if you have a label not included in existing models or if you have :\n",
    "- if you do need to cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Incantation</th>\n",
       "      <th>Type</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Light</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summoning Charm</td>\n",
       "      <td>Accio</td>\n",
       "      <td>Charm</td>\n",
       "      <td>Summons an object</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Age Line</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Charm</td>\n",
       "      <td>Prevents people above or below a certain age f...</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name Incantation   Type  \\\n",
       "0  Summoning Charm       Accio  Charm   \n",
       "1         Age Line     Unknown  Charm   \n",
       "\n",
       "                                              Effect Light  \n",
       "0                                  Summons an object  None  \n",
       "1  Prevents people above or below a certain age f...  Blue  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spells_df = pd.read_csv('./archive/Spells.csv', sep=\";\")\n",
    "spells_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check that spells exist in our films dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spells(row):\n",
    "    spells = spells_df[spells_df.Incantation.isna() == False].Incantation.unique().tolist()\n",
    "    identified_spells = []\n",
    "    for spell in spells:\n",
    "        if spell in row.Sentence:\n",
    "            identified_spells.append(spell)\n",
    "    row['identified_spells'] = ', '.join(identified_spells) if len(identified_spells) > 0 else ''\n",
    "    return row\n",
    "films_spells = films_df.apply(find_spells, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells_df.Incantation = spells_df.Incantation.fillna(\"\")\n",
    "spells = spells_df.Incantation.unique().tolist()\n",
    "list_spells = []\n",
    "for spell in spells:\n",
    "    split_spells = spell.split()\n",
    "    patterns = []\n",
    "    for sp in split_spells:\n",
    "        patterns.append({\"LOWER\": f\"{sp.lower()}\"})\n",
    "    spell_dict = {\"label\": \"TEXT\", \"pattern\": patterns, \"id\": f\"{'-'.join(spell.split())}\"}\n",
    "    list_spells.append(spell_dict)\n",
    "list_spells[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list_spells vs all_names\n",
    "# test_nlp = spacy.blank('en')\n",
    "# ruler = test_nlp.add_pipe(\"entity_ruler\") \n",
    "\n",
    "# ruler.add_patterns(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in range(1,8):\n",
    "    df = pd.read_csv(f'./archive/hp{i}.csv')\n",
    "    dfs.append(df)\n",
    "hp_dfs = pd.concat(dfs)\n",
    "hp_dfs[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spells_data(df, column_name):\n",
    "    spell_nlp = spacy.blank(\"en\")\n",
    "    matcher = Matcher(spell_nlp.vocab)\n",
    "    data_list = []\n",
    "    for index, row in df[df.identified_spells.str.len() > 1].iterrows():\n",
    "        spells = row.identified_spells.split(',')\n",
    "        \n",
    "        for spell in spells:\n",
    "            pattern = [{\"TEXT\": sp} for sp in spell.split()]\n",
    "            matcher.add(spell, [pattern])\n",
    "            doc = spell_nlp(row[f'{column_name}'])\n",
    "            matches = matcher(doc, as_spans=True)\n",
    "            for span in matches:\n",
    "                entry =(row[f'{column_name}'], {\"entities\": [span.start_char, span.end_char, \"SPELL\"]})\n",
    "                data_list.append(entry)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have spells within our scripts! Our next step is to extract this data as training data. Let's adapt our code from above here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'add_patterns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-813958b95b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mspell_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"SPELL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pattern\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{'-'.join(spell.split())}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlist_spells\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspell_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mspell_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining_spells\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'English' object has no attribute 'add_patterns'"
     ]
    }
   ],
   "source": [
    "#### TRAINING DATA CODE\n",
    "training_data = []\n",
    "spell_nlp = spacy.blank(\"en\")\n",
    "\n",
    "spell_nlp.add_pipe(\"ner\", name=\"spell_ner\") #before or after to position pipe spells and potions\n",
    "spell_ner = spell_nlp.get_pipe(\"spell_ner\")\n",
    "spell_ner.add_label(\"SPELL\")\n",
    "\n",
    "spells_df.Incantation = spells_df.Incantation.fillna(\"\")\n",
    "spells = spells_df.Incantation.unique().tolist()\n",
    "list_spells = []\n",
    "for spell in spells:\n",
    "    split_spells = spell.split()\n",
    "    patterns = []\n",
    "    for sp in split_spells:\n",
    "        patterns.append({\"LOWER\": f\"{sp.lower()}\"})\n",
    "    spell_dict = {\"label\": \"SPELL\", \"pattern\": patterns, \"id\": f\"{'-'.join(spell.split())}\"}\n",
    "    list_spells.append(spell_dict)\n",
    "spell_nlp.add_patterns(all_names)\n",
    "\n",
    "def training_spells(row):\n",
    "    # Let's also add in blank_nlp\n",
    "    sentences = nltk.sent_tokenize(row.Sentence.lower())\n",
    "    for sentence in sentences:\n",
    "        spacy_full = spell_nlp(sentence)\n",
    "        list_entities = []\n",
    "        for token in spacy_full.ents:\n",
    "            print(token)\n",
    "            list_entities.append([token.start_char, token.end_char, token.label_])\n",
    "        if len(list_entities) > 0:\n",
    "            entry = (sentence,{\"entities\": list_entities})\n",
    "            training_data.append(entry)\n",
    "    return row\n",
    "films_spells[films_spells.identified_spells.str.len() > 1].apply(evaluate_spacy_models, axis=1)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data =build_spells_data(films_spells, 'Sentence')\n",
    "evaluation_data = build_spells_data(hp_spells, 'dialog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique names and create our rules\n",
    "names = list(set(first_names) | set(last_names))\n",
    "unique_names = list(set(names) | set(full_names))\n",
    "list_names = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": f\"{name}\"}], \"id\": f\"{name}\"} for name in unique_names if len(name.split(' ')) == 1]\n",
    "\n",
    "list_full_names = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": f\"{name.split(' ')[0]}\"}, {\"LOWER\": f\"{name.split(' ')[1]}\"}], \"id\": f\"{'-'.join(name.split(' '))}\"} for name in full_names if len(name.split(' ')) > 1]\n",
    "\n",
    "all_names = list_names + list_full_names\n",
    "print('example rule', all_names[0])\n",
    "\n",
    "# Load our models and pass our rules\n",
    "full_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = full_nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(all_names)\n",
    "\n",
    "blank_nlp = spacy.blank(\"en\")\n",
    "ruler = blank_nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def build_person_data(df, column_name):\n",
    "    # Let's also add in blank_nlp\n",
    "    data_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentences = nltk.sent_tokenize(row[f'{column_name}'])\n",
    "        for sentence in sentences:\n",
    "            spacy_full = blank_nlp(sentence)\n",
    "            list_entities = []\n",
    "            for token in spacy_full.ents:\n",
    "                list_entities.append([token.start_char, token.end_char, token.label_])\n",
    "            if len(list_entities) > 0:\n",
    "                entry = (sentence,{\"entities\": list_entities})\n",
    "                data_list.append(entry)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_train_data = build_person_data(films_df, 'Sentence')\n",
    "ppl_valid_data = build_person_data(hp_dfs[hp_dfs.dialog.isna() == False], 'dialog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "def build_spacy_data(data, file_name):\n",
    "\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in data: # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for entity in annot[\"entities\"]:\n",
    "            start = entity[0]\n",
    "            end= entity[1]\n",
    "            label = entity[2]\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(f\"./{file_name}.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_spacy_data(ppl_train_data, 'train_person')\n",
    "build_spacy_data(ppl_valid_data, 'valid_person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_spells(row):\n",
    "    # Let's also add in blank_nlp\n",
    "    \n",
    "    doc = spell_nlp(row.Sentence)\n",
    "    print(doc)\n",
    "    matches = matcher(doc)\n",
    "    print(matches)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = spell_nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        print(match_id, string_id, start, end, span.text)\n",
    "    \n",
    "films_spells[films_spells.identified_spells.str.len() > 1][0:15].apply(evaluate_spacy_models, axis=1)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = spacy.load('./output/model-best')\n",
    "model_best.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Harry_Potter\n",
    "# test_text = \"The series continues with Harry Potter and the Chamber of Secrets, describing Harry's second year at Hogwarts. He and his friends investigate a 50-year-old mystery that appears uncannily related to recent sinister events at the school. Ron's younger sister, Ginny Weasley, enrols in her first year at Hogwarts, and finds an old notebook in her belongings which turns out to be the diary of a previous student, Tom Marvolo Riddle, written during World War II. He is later revealed to be Voldemort's younger self, who is bent on ridding the school of 'mudbloods', a derogatory term describing wizards and witches of non-magical parentage. The memory of Tom Riddle resides inside of the diary and when Ginny begins to confide in the diary, Voldemort is able to possess her. Through the diary, Ginny acts on Voldemort's orders and unconsciously opens the 'Chamber of Secrets', unleashing an ancient monster, later revealed to be a basilisk, which begins attacking students at Hogwarts. It kills those who make direct eye contact with it and petrifies those who look at it indirectly. The book also introduces a new Defence Against the Dark Arts teacher, Gilderoy Lockhart, a highly cheerful, self-conceited wizard with a pretentious facade, later turning out to be a fraud. Harry discovers that prejudice exists in the Wizarding World through delving into the school's history, and learns that Voldemort's reign of terror was often directed at wizards and witches who were descended from Muggles. Harry also learns that his ability to speak the snake language Parseltongue is rare and often associated with the Dark Arts. When Hermione is attacked and petrified, Harry and Ron finally piece together the puzzles and unlock the Chamber of Secrets, with Harry destroying the diary for good and saving Ginny, and, as they learn later, also destroying a part of Voldemort's soul. The end of the book reveals Lucius Malfoy, Draco's father and rival of Ron and Ginny's father, to be the culprit who slipped the book into Ginny's belongings.\"\n",
    "test_text = \"Ron Weasley: Wingardium Leviosa! Hermione Granger: You're saying it wrong. It's Wing-gar-dium Levi-o-sa, make the 'gar' nice and long. Ron Weasley: You do it, then, if you're so clever\"\n",
    "\n",
    "test_text = \"\"\"53. Imperio - Makes target obey every command But only for really, really funny pranks. 52. Piertotum Locomotor - Animates statues On one hand, this is awesome. On the other, someone would use this to scare me.\n",
    "\n",
    "51. Aparecium - Make invisible ink appear\n",
    "\n",
    "Your notes will be so much cooler.\n",
    "\n",
    "50. Defodio - Carves through stone and steel\n",
    "\n",
    "Sometimes you need to get the eff out of there.\n",
    "\n",
    "49. Descendo - Moves objects downward\n",
    "\n",
    "You'll never have to get a chair to reach for stuff again.\n",
    "\n",
    "48. Specialis Revelio - Reveals hidden magical properties in an object\n",
    "\n",
    "I want to know what I'm eating and if it's magical.\n",
    "\n",
    "47. Meteolojinx Recanto - Ends effects of weather spells\n",
    "\n",
    "Otherwise, someone could make it sleet in your bedroom forever.\n",
    "\n",
    "46. Cave Inimicum/Protego Totalum - Strengthens an area's defenses\n",
    "\n",
    "Helpful, but why are people trying to break into your campsite?\n",
    "\n",
    "45. Impedimenta - Freezes someone advancing toward you\n",
    "\n",
    "\"Stop running at me! But also, why are you running at me?\"\n",
    "\n",
    "44. Obscuro - Blindfolds target\n",
    "\n",
    "Finally, we don't have to rely on \"No peeking.\"\n",
    "\n",
    "43. Reducto - Explodes object\n",
    "\n",
    "The \"raddest\" of all spells.\n",
    "\n",
    "42. Anapneo - Clears someone's airway\n",
    "\n",
    "This could save a life, but hopefully you won't need it.\n",
    "\n",
    "41. Locomotor Mortis - Leg-lock curse\n",
    "\n",
    "Good for footraces and Southwest Airlines flights.\n",
    "\n",
    "40. Geminio - Creates temporary, worthless duplicate of any object\n",
    "\n",
    "You could finally live your dream of lying on a bed of marshmallows, and you'd only need one to start.\n",
    "\n",
    "39. Aguamenti - Shoot water from wand\n",
    "\n",
    "No need to replace that fire extinguisher you never bought.\n",
    "\n",
    "38. Avada Kedavra - The Killing Curse\n",
    "\n",
    "One word: bugs.\n",
    "\n",
    "37. Repelo Muggletum - Repels Muggles\n",
    "\n",
    "Sounds elitist, but seriously, Muggles ruin everything. Take it from me, a Muggle.\n",
    "\n",
    "36. Stupefy - Stuns target\n",
    "\n",
    "Since this is every other word of the \"Deathly Hallows\" script, I think it's pretty useful.\"\"\"\n",
    "doc = model_best(test_text)\n",
    "for token in doc.ents:\n",
    "    print('best', token.text, token.label_)\n",
    "\n",
    "# doc = full_nlp(test_text)\n",
    "# for token in doc.ents:\n",
    "#     print('full', token.text, token.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have that working let's take a look at the spaCy docs for training a model https://spacy.io/usage/training#ner\n",
    "\n",
    "> When training a model, we don’t just want it to memorize our examples – we want it to come up with a theory that can be generalized across unseen data. After all, we don’t just want the model to learn that this one instance of “Amazon” right here is a company – we want it to learn that “Amazon”, in contexts like this, is most likely a company. That’s why the training data should always be representative of the data we want to process. A model trained on Wikipedia, where sentences in the first person are extremely rare, will likely perform badly on Twitter. Similarly, a model trained on romantic novels will likely perform badly on legal text.\n",
    "\n",
    "*Annotation Tools*\n",
    "\n",
    "https://bohemian.ai/blog/text-annotation-tools-which-one-pick-2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hopefully now have our baseconfig file before we run it we need to save our corpora and let spaCy know where it exists.\n",
    "\n",
    "Let's follow the instructions in this blog post https://towardsdatascience.com/using-spacy-3-0-to-build-a-custom-ner-model-c9256bea098 and this one https://medium.com/analytics-vidhya/custom-named-entity-recognition-ner-model-with-spacy-3-in-four-steps-7e903688d51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "\n",
    "https://en.wikipedia.org/wiki/Catastrophic_interference\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c11445b2f880dceb70b92bfd87cba8fb4787433921a65f450e3d775bea6b0bcf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ner_hum_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}